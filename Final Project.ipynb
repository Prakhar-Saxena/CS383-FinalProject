{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teammates: Prakhar Saxena, Stephen Hansen, Tharindu Mendis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot as dot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Column Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_data(df):\n",
    "    for feature_split in feature_modifications:\n",
    "        df = feature_split(df)\n",
    "    x = df[feature_cols]\n",
    "    full_cols = feature_cols[:]\n",
    "    full_cols.append('Category')\n",
    "    print(feature_cols)\n",
    "    df_ = df[full_cols]\n",
    "    y = output_col_mod(df)\n",
    "    return x, y, df_\n",
    "\n",
    "def date_time_split(df):\n",
    "    def time_in_mins_since_midnight(time):\n",
    "        return (time.hour * 60) + time.minute\n",
    "\n",
    "    df['Time'] = pd.to_datetime(df['Dates']).dt.time.map(time_in_mins_since_midnight)\n",
    "\n",
    "    def time_bin(hour):\n",
    "        for bin_ in time_bins:\n",
    "            if hour < bin_:\n",
    "                return time_bins[bin_]\n",
    "\n",
    "    df['Time_Class'] = pd.to_datetime(df['Dates']).dt.hour.map(time_bin)\n",
    "    return df\n",
    "\n",
    "def day_map(df):\n",
    "    df = df.replace({\"DayOfWeek\": day_mapping})\n",
    "    return df\n",
    "\n",
    "def round_xy(df):\n",
    "    if 'X' in feature_cols:\n",
    "        df = df.round({'X': 2})\n",
    "    if 'Y' in feature_cols:\n",
    "        df = df.round({'Y': 2})\n",
    "    return df\n",
    "\n",
    "def drop_na(df):\n",
    "    return df.dropna(subset=feature_cols)\n",
    "\n",
    "# Modify Category to only include the top 5 most common crime categories \n",
    "# and everything else as other. Then assign values to each\n",
    "def output_col_mod(df):\n",
    "    df = df[['Category']]\n",
    "    cols = output_col_mapping.keys()\n",
    "    df = df.Category.map(output_col_mapping).fillna(output_col_mapping[\"OTHER OFFENSES\"]).astype(int)\n",
    "    df = df.to_frame().reset_index()\n",
    "    df = df[['Category']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'datasets/'\n",
    "\n",
    "# Include any columns that might be produced as a result of any feature modification functions\n",
    "# All columns will be numeric and this is being enforced\n",
    "# X and Y will be automatically rounded if in feature_cols\n",
    "# Drop_na should always be run last in feature modifications\n",
    "feature_cols = ['Time', 'Time_Class', 'DayOfWeek', 'X', 'Y']\n",
    "feature_modifications = [\n",
    "    date_time_split,\n",
    "    day_map,\n",
    "    round_xy,\n",
    "    drop_na\n",
    "]\n",
    "\n",
    "\n",
    "output_col = 'Category'\n",
    "\n",
    "# How many different classifications to make?\n",
    "output_col_mapping = {\n",
    "    'LARCENY/THEFT': 0, \n",
    "    'NON-CRIMINAL': 1, \n",
    "    'ASSAULT': 2,\n",
    "    'DRUG/NARCOTIC': 3,\n",
    "    'VEHICLE THEFT': 4,\n",
    "#     Includes other offenses and all offenses not included above\n",
    "    'OTHER OFFENSES': 5,\n",
    "}\n",
    "\n",
    "# {'LARCENY/THEFT': 174900, 'OTHER OFFENSES': 126182, 'NON-CRIMINAL': 92304, 'ASSAULT': 76876, \n",
    "# 'DRUG/NARCOTIC': 53971, 'VEHICLE THEFT': 53781, 'VANDALISM': 44725, 'WARRANTS': 42214, \n",
    "# 'BURGLARY': 36755, 'SUSPICIOUS OCC': 31414, 'MISSING PERSON': 25989, 'ROBBERY': 23000, \n",
    "# 'FRAUD': 16679, 'FORGERY/COUNTERFEITING': 10609, 'SECONDARY CODES': 9985, 'WEAPON LAWS': 8555, \n",
    "# 'PROSTITUTION': 7484, 'TRESPASS': 7326, 'STOLEN PROPERTY': 4540, 'SEX OFFENSES FORCIBLE': 4388, \n",
    "# 'DISORDERLY CONDUCT': 4320, 'DRUNKENNESS': 4280, 'RECOVERED VEHICLE': 3138, 'KIDNAPPING': 2341, \n",
    "# 'DRIVING UNDER THE INFLUENCE': 2268, 'RUNAWAY': 1946, 'LIQUOR LAWS': 1903, 'ARSON': 1513, \n",
    "# 'LOITERING': 1225, 'EMBEZZLEMENT': 1166, 'SUICIDE': 508, 'FAMILY OFFENSES': 491, 'BAD CHECKS': 406, \n",
    "# 'BRIBERY': 289, 'EXTORTION': 256, 'SEX OFFENSES NON FORCIBLE': 148, 'GAMBLING': 146, \n",
    "# 'PORNOGRAPHY/OBSCENE MAT': 22, 'TREA': 6}\n",
    "\n",
    "\n",
    "# Keys are evaluated as: hour is less than key\n",
    "# Cycle runs from midnight 00:00 to 23:59\n",
    "time_bins = {\n",
    "    4: 0, # Before 4am is 0\n",
    "    6: 1, # Before 6am is 1\n",
    "    12: 2, # Before 12pm is 2\n",
    "    18: 3, # Before 4pm is 3\n",
    "    24: 4, # Before midnight is 4\n",
    "}\n",
    "\n",
    "day_mapping = {\n",
    "    'Sunday': 0,\n",
    "    'Monday': 1,\n",
    "    'Tuesday': 2,\n",
    "    'Wednesday': 3,\n",
    "    'Thursday': 4,\n",
    "    'Friday': 5,\n",
    "    'Saturday': 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(x, y, shuffle=True, test_size=0.2):\n",
    "    return train_test_split(x, y, test_size=test_size, random_state=0, shuffle=shuffle)\n",
    "\n",
    "# Standardize by subtracting column mean and divide by the standard deviation of the column\n",
    "def standardize_data(training_data, testing_data=None):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(training_data.to_numpy())\n",
    "    training_data = scaler.transform(training_data.to_numpy())\n",
    "    testing_data = scaler.transform(testing_data.to_numpy())\n",
    "    \n",
    "    return training_data, testing_data\n",
    "\n",
    "# Prepare and Get test train data\n",
    "def get_data():\n",
    "    # Fixing random state for reproducibility\n",
    "    np.random.seed(0)\n",
    "\n",
    "    train = pd.read_csv(dataset_dir + 'train.csv')\n",
    "\n",
    "    x, y, df = modify_data(train)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = test_train_split(x, y)\n",
    "\n",
    "    # Standardize training and testing data using training\n",
    "    # (Feature columns only)\n",
    "    X_train, X_test = standardize_data(X_train, X_test)\n",
    "\n",
    "    # Fixing random state for reproducibility again cause optional preparation steps\n",
    "    # might have random steps\n",
    "    np.random.seed(0)\n",
    "\n",
    "    return X_train, X_test, y_train.to_numpy(), y_test.to_numpy(), df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'Time_Class', 'DayOfWeek', 'X', 'Y']\n",
      "[[-0.72389912 -0.55441529  0.97616645 -0.2431428   0.04298224]\n",
      " [-0.19307212  0.23012869 -1.54299048  0.08688813 -0.00188857]\n",
      " [-0.86359043 -0.55441529 -1.54299048  0.74694999  0.06541764]\n",
      " ...\n",
      " [ 1.10478719  1.01467266 -0.53532771  0.41691906  0.02054683]\n",
      " [-0.36832122 -0.55441529 -1.54299048  0.41691906  0.04298224]\n",
      " [-0.05084096  0.23012869  1.47999784  1.07698092 -0.11406559]]\n",
      "[[ 0.57142035  0.23012869  0.97616645  1.07698092 -0.02432397]\n",
      " [ 0.72381088  1.01467266 -0.03149632 -0.2431428  -0.02432397]\n",
      " [ 0.42664935  0.23012869 -0.53532771  0.41691906 -0.04675938]\n",
      " ...\n",
      " [-0.57150859 -0.55441529  1.47999784  1.40701185 -0.09163019]\n",
      " [-0.15751433  0.23012869 -1.03915909  0.41691906  0.02054683]\n",
      " [-0.54611017 -0.55441529  1.47999784  0.74694999  0.02054683]]\n",
      "[[0]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [3]\n",
      " [2]]\n",
      "[[5]\n",
      " [5]\n",
      " [5]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "        Time  Time_Class  DayOfWeek       X      Y                Category\n",
      "0       1433           4          3 -122.43  37.77                WARRANTS\n",
      "1       1433           4          3 -122.43  37.77          OTHER OFFENSES\n",
      "2       1413           4          3 -122.42  37.80          OTHER OFFENSES\n",
      "3       1410           4          3 -122.43  37.80           LARCENY/THEFT\n",
      "4       1410           4          3 -122.44  37.77           LARCENY/THEFT\n",
      "...      ...         ...        ...     ...    ...                     ...\n",
      "878044    15           0          1 -122.46  37.71                 ROBBERY\n",
      "878045     1           0          1 -122.45  37.73           LARCENY/THEFT\n",
      "878046     1           0          1 -122.40  37.78           LARCENY/THEFT\n",
      "878047     1           0          1 -122.39  37.78               VANDALISM\n",
      "878048     1           0          1 -122.39  37.74  FORGERY/COUNTERFEITING\n",
      "\n",
      "[878049 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, df = get_data()\n",
    "# df is pandas dataframe before standardizing data\n",
    "# y_test and X_test are Category data and style can be found above\n",
    "# X_train and y_train cols are feature cols in the order\n",
    "# Currently Feature Cols are: ['Time', 'Time_Class', 'DayOfWeek', 'X', 'Y']\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'Time_Class', 'DayOfWeek', 'X', 'Y']\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, df = get_data()\n",
    "# df is pandas dataframe before standardizing data\n",
    "# y_test and X_test are Category data and style can be found above\n",
    "# X_train and y_train cols are feature cols in the order\n",
    "# Currently Feature Cols are: ['Time', 'Time_Class', 'DayOfWeek', 'X', 'Y']\n",
    "def get_entropy(y_values):\n",
    "    h, w = y_values.shape\n",
    "    unique, counts = np.unique(y_values, return_counts=True)\n",
    "    entropy = 0\n",
    "    for count in counts:\n",
    "        P = count/h\n",
    "        entropy += -P*np.log2(P)\n",
    "    return entropy\n",
    "\n",
    "def get_information_gains(X_values, y_values):\n",
    "    h, w = X_values.shape\n",
    "    gains = []\n",
    "    for x in range(w): # x being our current feature\n",
    "        col = X_values[:,x]\n",
    "        unique, counts = np.unique(col, return_counts=True)\n",
    "        totals = dict(zip(unique, counts))\n",
    "        entropy = 0\n",
    "        for t in totals:\n",
    "            count = totals[t]\n",
    "            P = count/h\n",
    "            mask = col == t\n",
    "            y_masked = y_values[mask]\n",
    "            entropy += P*get_entropy(y_masked)\n",
    "        gain = get_entropy(y_values) - entropy\n",
    "        gains.append((x, gain))\n",
    "        \n",
    "    return sorted(gains, key=lambda x : x[1], reverse=True)\n",
    "\n",
    "def group_dataset_by(X_values, y_values, index):\n",
    "    h, w = X_values.shape\n",
    "    col = X_values[:,index]\n",
    "    unique, counts = np.unique(col, return_counts=True)\n",
    "    X_filtered = np.delete(X_values, index, axis=1)\n",
    "    groupings = []\n",
    "    for u in unique: # loop over all unique groups\n",
    "        mask = col == u\n",
    "        X_masked = X_filtered[mask]\n",
    "        y_masked = y_values[mask]\n",
    "        groupings.append((X_masked, y_masked))\n",
    "    return groupings, unique\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, label):\n",
    "        self.label = label\n",
    "        self.connections = []\n",
    "        \n",
    "    def addTree(self, edge_value, tree):\n",
    "        self.connections.append((edge_value, tree))\n",
    "        \n",
    "    def __str__(self, level=0):\n",
    "        ret = \"\\t\"*level+repr(self.label)+\"\\n\"\n",
    "        for connection in self.connections:\n",
    "            if connection[1] is not None:\n",
    "                ret += connection[1].__str__(level+1)\n",
    "        return ret\n",
    "    \n",
    "def build_tree_recurse(X, y, cols):\n",
    "    if len(cols) == 1:\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        i = np.argmax(counts)\n",
    "        return TreeNode(unique[i])\n",
    "    entropy = get_entropy(y)\n",
    "    #print(f\"The entropy is {entropy}\")\n",
    "    gains = get_information_gains(X, y)\n",
    "    #print(f\"The information gains of the training set are {gains}\")\n",
    "    best_feature = gains[0][0]\n",
    "    #print(f\"The best feature is {cols[best_feature]}\")\n",
    "    tree = TreeNode(cols[best_feature])\n",
    "    columns = cols[:]\n",
    "    columns.pop(best_feature)\n",
    "    new_groups, unique = group_dataset_by(X, y, best_feature)\n",
    "    for i in range(len(new_groups)):\n",
    "        g = new_groups[i]\n",
    "        u = unique[i]\n",
    "        child_tree = build_tree_recurse(g[0], g[1], columns)\n",
    "        tree.addTree(u, child_tree)\n",
    "    return tree\n",
    "\n",
    "def predict_DT(X, y, labels, tree):\n",
    "    h, w = X.shape\n",
    "    num_right = 0\n",
    "    for i in range(h):\n",
    "        X_row = X[i,:]\n",
    "        y_val = y[i,0]\n",
    "        curr_node = tree\n",
    "        while len(curr_node.connections) > 0:\n",
    "            index = labels.index(curr_node.label)\n",
    "            val = X_row[index]\n",
    "            for c in curr_node.connections:\n",
    "                edge = c[0]\n",
    "                new_node = c[1]\n",
    "                if edge == X_row[index]:\n",
    "                    break\n",
    "            curr_node = new_node\n",
    "        y_pred = curr_node.label\n",
    "        if y_val == y_pred:\n",
    "            num_right += 1\n",
    "    return num_right / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Time', 'Time_Class', 'DayOfWeek', 'X', 'Y']\n",
    "tree = build_tree_recurse(X_train, y_train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy = 0.6252457508765886\n",
      "Testing accuracy = 0.4163487272934343\n"
     ]
    }
   ],
   "source": [
    "accuracy_train = predict_DT(X_train, y_train, labels, tree)\n",
    "print(f\"Training accuracy = {accuracy_train}\")\n",
    "accuracy_test = predict_DT(X_test, y_test, labels, tree)\n",
    "print(f\"Testing accuracy = {accuracy_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'X'\n",
      "\t'Time_Class'\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t3\n",
      "\t'DayOfWeek'\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t0\n",
      "\t'Time_Class'\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t5\n",
      "\t'Time_Class'\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t4\n",
      "\t'Y'\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t4\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t'DayOfWeek'\n",
      "\t\t'Time_Class'\n",
      "\t\t\t1\n",
      "\t\t'Time_Class'\n",
      "\t\t\t3\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t'DayOfWeek'\n",
      "\t\t'Time_Class'\n",
      "\t\t\t2\n",
      "\t\t'Time_Class'\n",
      "\t\t\t2\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t'DayOfWeek'\n",
      "\t\t'Time_Class'\n",
      "\t\t\t4\n",
      "\t\t'Y'\n",
      "\t\t\t1\n",
      "\t\t\t5\n",
      "\t\t'Y'\n",
      "\t\t\t3\n",
      "\t\t\t5\n",
      "\t\t\t5\n",
      "\t\t'Y'\n",
      "\t\t\t2\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t1\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t'Y'\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t4\n",
      "\t\t\t1\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t5\n",
      "\t\t\t2\n",
      "\t\t\t5\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t0\n",
      "\t\t\t5\n",
      "\t\t\t5\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t5\n",
      "\t\t\t0\n",
      "\t'Y'\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t5\n",
      "\t\t\t3\n",
      "\t\t'Time_Class'\n",
      "\t\t\t1\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t5\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t4\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t5\n",
      "\t\t\t5\n",
      "\t\t\t3\n",
      "\t\t\t5\n",
      "\t\t\t1\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t5\n",
      "\t\t\t3\n",
      "\t\t\t5\n",
      "\t\t\t5\n",
      "\t\t\t3\n",
      "\t\t\t0\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t5\n",
      "\t\t\t5\n",
      "\t\t\t0\n",
      "\t\t\t5\n",
      "\t\t\t2\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t1\n",
      "\t\t\t5\n",
      "\t\t\t5\n",
      "\t\t\t5\n",
      "\t\t\t3\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t5\n",
      "\t\t\t5\n",
      "\t\t\t1\n",
      "\t\t\t5\n",
      "\t\t\t0\n",
      "\t\t'Time_Class'\n",
      "\t\t\t2\n",
      "\t'Y'\n",
      "\t\t'Time_Class'\n",
      "\t\t\t4\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t5\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t2\n",
      "\t\t\t5\n",
      "\t\t\t3\n",
      "\t\t\t5\n",
      "\t\t\t5\n",
      "\t\t\t3\n",
      "\t\t\t5\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t2\n",
      "\t\t\t5\n",
      "\t\t\t5\n",
      "\t\t\t5\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t'Y'\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t1\n",
      "\t\t\t5\n",
      "\t\t\t5\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t5\n",
      "\t\t\t5\n",
      "\t\t\t1\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t0\n",
      "\t\t\t0\n",
      "\t\t\t5\n",
      "\t\t\t1\n",
      "\t\t\t0\n",
      "\t\t\t1\n",
      "\t\t'DayOfWeek'\n",
      "\t\t\t5\n",
      "\t\t\t5\n",
      "\t\t\t2\n",
      "\t\t\t2\n",
      "\t\t'Time_Class'\n",
      "\t\t\t2\n",
      "\t'DayOfWeek'\n",
      "\t\t'Y'\n",
      "\t\t\t5\n",
      "\t\t\t3\n",
      "\t\t'Time_Class'\n",
      "\t\t\t1\n",
      "\t\t'Time_Class'\n",
      "\t\t\t5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tree.connections[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn testing accuracy = 0.6252457508765886\n",
      "Sklearn testing accuracy = 0.41528386766129494\n"
     ]
    }
   ],
   "source": [
    "num_right = 0\n",
    "for i in range(y_train.shape[0]):\n",
    "    if y_train[i,:] == y_pred_train[i]:\n",
    "        num_right += 1\n",
    "sklearn_accuracy = num_right / y_train.shape[0]\n",
    "print(f\"Sklearn testing accuracy = {sklearn_accuracy}\")\n",
    "num_right = 0\n",
    "for i in range(y_test.shape[0]):\n",
    "    if y_test[i,:] == y_pred_test[i]:\n",
    "        num_right += 1\n",
    "sklearn_accuracy = num_right / y_test.shape[0]\n",
    "print(f\"Sklearn testing accuracy = {sklearn_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, df = get_data()\n",
    "# df is pandas dataframe before standardizing data\n",
    "# y_test and X_test are Category data and style can be found above\n",
    "# X_train and y_train cols are feature cols in the order\n",
    "# Currently Feature Cols are: ['Time', 'Time_Class', 'DayOfWeek', 'X', 'Y']\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, df = get_data()\n",
    "# df is pandas dataframe before standardizing data\n",
    "# y_test and X_test are Category data and style can be found above\n",
    "# X_train and y_train cols are feature cols in the order\n",
    "# Currently Feature Cols are: ['Time', 'Time_Class', 'DayOfWeek', 'X', 'Y']\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
