{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: KMeans Clustering with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teammates: Prakhar Saxena, Stephen Hansen, Tharindu Mendis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot as dot\n",
    "from numpy.linalg import svd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "from statistics import mode, StatisticsError\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Column Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_data(df):\n",
    "    for feature_split in feature_modifications:\n",
    "        df = feature_split(df)\n",
    "    x = df[feature_cols]\n",
    "    full_cols = feature_cols[:]\n",
    "    full_cols.append('Category')\n",
    "    df_ = df[full_cols]\n",
    "    y = output_col_mod(df)\n",
    "    return x, y, df_\n",
    "\n",
    "def date_time_split(df):\n",
    "    def time_in_mins_since_midnight(time):\n",
    "        return (time.hour * 60) + time.minute\n",
    "\n",
    "    df['Time'] = pd.to_datetime(df['Dates']).dt.time.map(time_in_mins_since_midnight)\n",
    "\n",
    "    def time_bin(hour):\n",
    "        for bin_ in time_bins:\n",
    "            if hour < bin_:\n",
    "                return time_bins[bin_]\n",
    "\n",
    "    df['Time_Class'] = pd.to_datetime(df['Dates']).dt.hour.map(time_bin)\n",
    "    return df\n",
    "\n",
    "def day_map(df):\n",
    "    df = df.replace({\"DayOfWeek\": day_mapping})\n",
    "    return df\n",
    "\n",
    "def round_xy(df):\n",
    "    if 'X' in feature_cols:\n",
    "        df = df.round({'X': 2})\n",
    "    if 'Y' in feature_cols:\n",
    "        df = df.round({'Y': 2})\n",
    "    return df\n",
    "\n",
    "def drop_na(df):\n",
    "    return df.dropna(subset=feature_cols)\n",
    "\n",
    "# Modify Category to only include the top 5 most common crime categories \n",
    "# and everything else as other. Then assign values to each\n",
    "def output_col_mod(df):\n",
    "    df = df[['Category']]\n",
    "    cols = output_col_mapping.keys()\n",
    "    df = df.Category.map(output_col_mapping).fillna(output_col_mapping[\"OTHER OFFENSES\"]).astype(int)\n",
    "    df = df.to_frame().reset_index()\n",
    "    df = df[['Category']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '../CS383_datasets/'\n",
    "\n",
    "# Include any columns that might be produced as a result of any feature modification functions\n",
    "# All columns will be numeric and this is being enforced\n",
    "# X and Y will be automatically rounded if in feature_cols\n",
    "# Drop_na should always be run last in feature modifications\n",
    "feature_cols = ['Time', 'Time_Class', 'DayOfWeek', 'X', 'Y']\n",
    "feature_modifications = [\n",
    "    date_time_split,\n",
    "    day_map,\n",
    "    round_xy,\n",
    "    drop_na\n",
    "]\n",
    "\n",
    "\n",
    "output_col = 'Category'\n",
    "\n",
    "# How many different classifications to make?\n",
    "output_col_mapping = {\n",
    "    'LARCENY/THEFT': 0, \n",
    "    'NON-CRIMINAL': 1, \n",
    "    'ASSAULT': 2,\n",
    "    'DRUG/NARCOTIC': 3,\n",
    "    'VEHICLE THEFT': 4,\n",
    "#     Includes other offenses and all offenses not included above\n",
    "    'OTHER OFFENSES': 5,\n",
    "}\n",
    "\n",
    "# {'LARCENY/THEFT': 174900, 'OTHER OFFENSES': 126182, 'NON-CRIMINAL': 92304, 'ASSAULT': 76876, \n",
    "# 'DRUG/NARCOTIC': 53971, 'VEHICLE THEFT': 53781, 'VANDALISM': 44725, 'WARRANTS': 42214, \n",
    "# 'BURGLARY': 36755, 'SUSPICIOUS OCC': 31414, 'MISSING PERSON': 25989, 'ROBBERY': 23000, \n",
    "# 'FRAUD': 16679, 'FORGERY/COUNTERFEITING': 10609, 'SECONDARY CODES': 9985, 'WEAPON LAWS': 8555, \n",
    "# 'PROSTITUTION': 7484, 'TRESPASS': 7326, 'STOLEN PROPERTY': 4540, 'SEX OFFENSES FORCIBLE': 4388, \n",
    "# 'DISORDERLY CONDUCT': 4320, 'DRUNKENNESS': 4280, 'RECOVERED VEHICLE': 3138, 'KIDNAPPING': 2341, \n",
    "# 'DRIVING UNDER THE INFLUENCE': 2268, 'RUNAWAY': 1946, 'LIQUOR LAWS': 1903, 'ARSON': 1513, \n",
    "# 'LOITERING': 1225, 'EMBEZZLEMENT': 1166, 'SUICIDE': 508, 'FAMILY OFFENSES': 491, 'BAD CHECKS': 406, \n",
    "# 'BRIBERY': 289, 'EXTORTION': 256, 'SEX OFFENSES NON FORCIBLE': 148, 'GAMBLING': 146, \n",
    "# 'PORNOGRAPHY/OBSCENE MAT': 22, 'TREA': 6}\n",
    "\n",
    "\n",
    "# Keys are evaluated as: hour is less than key\n",
    "# Cycle runs from midnight 00:00 to 23:59\n",
    "time_bins = {\n",
    "    4: 0, # Before 4am is 0\n",
    "    6: 1, # Before 6am is 1\n",
    "    12: 2, # Before 12pm is 2\n",
    "    18: 3, # Before 4pm is 3\n",
    "    24: 4, # Before midnight is 4\n",
    "}\n",
    "\n",
    "day_mapping = {\n",
    "    'Sunday': 0,\n",
    "    'Monday': 1,\n",
    "    'Tuesday': 2,\n",
    "    'Wednesday': 3,\n",
    "    'Thursday': 4,\n",
    "    'Friday': 5,\n",
    "    'Saturday': 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(x, y, shuffle=True, test_size=0.2):\n",
    "    return train_test_split(x, y, test_size=test_size, random_state=0, shuffle=shuffle)\n",
    "\n",
    "# Standardize by subtracting column mean and divide by the standard deviation of the column\n",
    "def standardize_data(training_data, testing_data=None, std_mean=False, numpy_array=False):\n",
    "    if std_mean:\n",
    "        std = np.std(training_data, axis=0, ddof=1)\n",
    "        mean = np.mean(training_data, axis=0)\n",
    "        if testing_data is None:\n",
    "            return (training_data - mean) / std, 0, std, mean\n",
    "        return (training_data - mean) / std, (testing_data - mean) / std, std, mean\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        if numpy_array:\n",
    "            scaler.fit(training_data)\n",
    "            training_data = scaler.transform(training_data)\n",
    "            testing_data = scaler.transform(testing_data)\n",
    "        else:\n",
    "            scaler.fit(training_data.to_numpy())\n",
    "            training_data = scaler.transform(training_data.to_numpy())\n",
    "            testing_data = scaler.transform(testing_data.to_numpy())\n",
    "        return training_data, testing_data\n",
    "\n",
    "\n",
    "# Prepare and Get test train data\n",
    "def get_data():\n",
    "    # Fixing random state for reproducibility\n",
    "    np.random.seed(0)\n",
    "\n",
    "    train = pd.read_csv(dataset_dir + 'train.csv')\n",
    "    \n",
    "    train = train.sample(n=1000)\n",
    "#     train = train\n",
    "#     train = train[train.Category.isin(['LARCENY/THEFT', 'NON-CRIMINAL', 'ASSAULT', 'DRUG/NARCOTIC', 'VEHICLE THEFT'])]\n",
    "\n",
    "    x, y, df = modify_data(train)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = test_train_split(x, y)\n",
    "\n",
    "    # Standardize training and testing data using training\n",
    "    # (Feature columns only)\n",
    "    X_train, X_test = standardize_data(X_train, X_test)\n",
    "\n",
    "    # Fixing random state for reproducibility again cause optional preparation steps\n",
    "    # might have random steps\n",
    "    np.random.seed(0)\n",
    "\n",
    "    return X_train, X_test, y_train.to_numpy(), y_test.to_numpy(), df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get face data as per professors code\n",
    "def fetch_face_data():\n",
    "    people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\n",
    "    mask = np.zeros(people.target.shape, dtype=np.bool)\n",
    "    for target in np.unique(people.target):\n",
    "        mask[np.where(people.target == target)[0][:50]] = 1\n",
    "    X_people = people.data[mask]\n",
    "    y_people = people.target[mask]\n",
    "    X_people = X_people/255\n",
    "    return X_people, y_people\n",
    "\n",
    "\n",
    "# Calculate euclidean distance\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum(np.square(a - b)))\n",
    "\n",
    "\n",
    "def manhattan_distance(a, b):\n",
    "    return np.sum(np.abs(a - b))\n",
    "\n",
    "\n",
    "# Get Principal Components Analysis Vectors and Vals using eigh.\n",
    "# I found eigh to have non complex vectors \n",
    "def pca_vv(data):\n",
    "    std_data, _, std, mean = standardize_data(data, std_mean=True)\n",
    "\n",
    "    # Calculate covariance matrix\n",
    "    cv = np.cov(std_data.T)\n",
    "\n",
    "    # Calculate the eigenvectors using eig on the covariance matrix\n",
    "    # vals, vectors = np.linalg.eig(cv)\n",
    "    vals, vectors = np.linalg.eigh(cv)\n",
    "\n",
    "    return std_data, vals, vectors, std, mean\n",
    "\n",
    "\n",
    "# Project and Restructure\n",
    "def pca_dr(std_data, vals, vectors, d, whitening=False):\n",
    "    # Take d highest eigenvectors and eigenvals\n",
    "    idx = vals.argsort()[-d:][::-1]\n",
    "    vals = vals[idx]\n",
    "    vectors_subset_ = vectors[:, idx]\n",
    "\n",
    "    # Take the dot product to get the projection matrix\n",
    "    z = np.dot(std_data, vectors_subset_)\n",
    "\n",
    "    # If whitening, then add a diagonal matrix\n",
    "    if whitening:\n",
    "        alpha = np.diag(1. / np.sqrt(vals))\n",
    "        z = np.dot(z, alpha)\n",
    "\n",
    "    r = np.dot(z, vectors_subset_.T)\n",
    "    return z, r\n",
    "\n",
    "\n",
    "def myKMeans(X, Y, k):\n",
    "    # Fixing random state for reproducibility\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Choosing k random rows to serve as the Intial Vectors\n",
    "    r = np.random.choice(X.shape[0], k, replace=False).tolist()\n",
    "\n",
    "    plot_data = []\n",
    "    rows = []\n",
    "\n",
    "    r = sorted(r, reverse=True)\n",
    "\n",
    "    for idx, val in enumerate(r):\n",
    "        rows.append({\"ref\": X[val], \"class_rows\": None, \"idx\": None, \"match_count\": 0})\n",
    "\n",
    "    # I decided to remove the reference vectors from the dataset\n",
    "    for idx, val in enumerate(r):\n",
    "        X = np.delete(X, val, axis=0)\n",
    "        Y = np.delete(Y, val, axis=0)\n",
    "\n",
    "    plot_data.append(rows)\n",
    "    purity = []\n",
    "\n",
    "    previous_change_sum = None\n",
    "    current_change_sum = None\n",
    "    test_break_condition = False\n",
    "    epsilon = pow(2, -23)\n",
    "    counter = 0\n",
    "    while True:\n",
    "        vec = copy.deepcopy(plot_data[-1])\n",
    "        for idx, vector in enumerate(vec):\n",
    "            vec[idx][\"class_rows\"] = None\n",
    "        # Using the euclidean distance to put each person in the proper reference vector set.\n",
    "        for master_idx, person in enumerate(X):\n",
    "            smallest = None\n",
    "            smallest_idx = 0\n",
    "            for idx, vector in enumerate(vec):\n",
    "                if smallest is None:\n",
    "                    smallest = euclidean_distance(person, vector[\"ref\"])\n",
    "                    smallest_idx = idx\n",
    "                else:\n",
    "                    new_small = euclidean_distance(person, vector[\"ref\"])\n",
    "                    if new_small < smallest:\n",
    "                        smallest_idx = idx\n",
    "                        smallest = new_small\n",
    "            if vec[smallest_idx][\"class_rows\"] is None:\n",
    "                vec[smallest_idx][\"class_rows\"] = person\n",
    "                vec[smallest_idx][\"idx\"] = [master_idx]\n",
    "            else:\n",
    "                vec[smallest_idx][\"class_rows\"] = np.vstack((vec[smallest_idx][\"class_rows\"], person))\n",
    "                vec[smallest_idx][\"idx\"].append(master_idx)\n",
    "\n",
    "        # Calculating purity\n",
    "        for idx, vector in enumerate(vec):\n",
    "            pos_one_count = 0\n",
    "            neg_one_count = 0\n",
    "            for y_idx in vector[\"idx\"]:\n",
    "                if Y[y_idx] == 1:\n",
    "                    pos_one_count += 1\n",
    "                else:\n",
    "                    neg_one_count += 1\n",
    "            if pos_one_count >= neg_one_count:\n",
    "                vec[idx][\"purity\"] = pos_one_count / len(vec[idx][\"idx\"])\n",
    "            else:\n",
    "                vec[idx][\"purity\"] = neg_one_count / len(vec[idx][\"idx\"])\n",
    "\n",
    "        purity_ = 0\n",
    "        for idx, vector in enumerate(vec):\n",
    "            purity_ += vec[idx][\"purity\"] * len(vec[idx][\"idx\"])\n",
    "        purity.append(purity_/len(Y))\n",
    "\n",
    "        # Using the manhattan distance to calculate the change of magnitude summation\n",
    "        for idx, vector_set in enumerate(vec):\n",
    "            vec[idx][\"ref\"] = np.mean(vector_set[\"class_rows\"], axis=0)\n",
    "            if current_change_sum is not None:\n",
    "                vec[idx][\"moc\"] = manhattan_distance(plot_data[-1][idx][\"ref\"], vec[idx][\"ref\"])\n",
    "\n",
    "        if current_change_sum is not None:\n",
    "            current_change_sum = 0\n",
    "            for idx, vector_set in enumerate(vec):\n",
    "                current_change_sum += vector_set[\"moc\"]\n",
    "\n",
    "        plot_data.append(vec)\n",
    "        if len(plot_data) >= 3:\n",
    "            plot_data = plot_data[-2:]\n",
    "\n",
    "        if current_change_sum is not None:\n",
    "            if test_break_condition:\n",
    "                # If change is less than epsilon, break\n",
    "                if abs(current_change_sum - previous_change_sum) < epsilon:\n",
    "                    break\n",
    "            else:\n",
    "                test_break_condition = True\n",
    "        else:\n",
    "            current_change_sum = 0\n",
    "\n",
    "        counter += 1\n",
    "        previous_change_sum = copy.deepcopy(current_change_sum)\n",
    "\n",
    "    #     My custom alogirthm to map to class labels based on classification\n",
    "    ret_val = []\n",
    "    total_count = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0}\n",
    "    c = []\n",
    "    for row in plot_data[-1]:\n",
    "        sub = np.take(Y, row[\"idx\"], 0)\n",
    "        m = np.bincount(sub[:, 0]).tolist()\n",
    "        for idx, val in enumerate(m):\n",
    "            total_count[idx] += val\n",
    "        c.append(m)\n",
    "        ret_val.append({\"ref\": row[\"ref\"], \"y\": None, \"purity\": row[\"purity\"]})\n",
    "\n",
    "    counts = []\n",
    "    finalized = {\n",
    "        0: False, 1: False, 2: False, 3: False, 4: False, 5: False\n",
    "    }\n",
    "    for q_idx, count in enumerate(c):\n",
    "        append_count = []\n",
    "        for w_idx, ro in enumerate(count):\n",
    "            append_count.append(round((ro/total_count[w_idx])*100, 2))\n",
    "        append_count = sorted(range(len(append_count)), key=lambda k: append_count[k])\n",
    "        counts.append(append_count)\n",
    "        s_idx = 5\n",
    "        while True:\n",
    "            found = False\n",
    "            if finalized[append_count.index(s_idx)]:\n",
    "                s_idx -= 1\n",
    "            else:\n",
    "                finalized[append_count.index(s_idx)] = True\n",
    "                ret_val[q_idx][\"y\"] = append_count.index(s_idx)\n",
    "                found = True\n",
    "            if found:\n",
    "                break\n",
    "    \n",
    "    return ret_val\n",
    "\n",
    "\n",
    "def test_model(refs, testing_X, testing_Y):\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    for idx, row in enumerate(testing_X):\n",
    "        smallest = None\n",
    "        pred = 0\n",
    "        for ref_idx, ref in enumerate(refs):\n",
    "            if smallest is None:\n",
    "                smallest = euclidean_distance(row, ref[\"ref\"])\n",
    "                pred = ref[\"y\"]\n",
    "            else:\n",
    "                new_small = euclidean_distance(row, ref[\"ref\"])\n",
    "                if new_small < smallest:\n",
    "                    pred = ref[\"y\"]\n",
    "                    smallest = new_small\n",
    "        if pred == 0 and testing_Y[idx] == 0:\n",
    "            correct += 1\n",
    "        elif pred == 1 and testing_Y[idx] == 1:\n",
    "            correct += 1\n",
    "        elif pred == 2 and testing_Y[idx] == 2:\n",
    "            correct += 1\n",
    "        elif pred == 3 and testing_Y[idx] == 3:\n",
    "            correct += 1\n",
    "        elif pred == 4 and testing_Y[idx] == 4:\n",
    "            correct += 1\n",
    "        elif pred == 5 and testing_Y[idx] == 5:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "\n",
    "#     precision = tp / (tp + fp)\n",
    "#     recall = tp / (tp + fn)\n",
    "#     f_measure = (2 * precision * recall) / (precision + recall)\n",
    "    accuracy = correct / (correct+wrong)\n",
    "#     print(\"Precision: {}\".format(precision * 100))\n",
    "#     print(\"Recall: {}\".format(recall * 100))\n",
    "#     print(\"F-measure: {}\".format(f_measure * 100))\n",
    "    print(\"Accuracy: {}\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using my K-Means Algorithm with PCA d=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCA\n",
      "Running K-Means Clustering\n",
      "Clustering has ended!\n",
      "\n",
      "Purity of Refs:\n",
      "{'ref': array([-0.21189379, -1.94959232,  0.66694336, -0.89039051,  0.02348225]), 'y': 2, 'purity': 0.8658536585365854}\n",
      "{'ref': array([ 2.71144642,  0.11812763,  0.1433275 ,  0.07438304, -0.13149775]), 'y': 4, 'purity': 0.8990825688073395}\n",
      "{'ref': array([-0.6977257 ,  0.75953069,  0.82042515,  0.0097729 ,  0.01055863]), 'y': 0, 'purity': 0.8721461187214612}\n",
      "{'ref': array([ 0.48067296,  0.37453129, -0.71843771, -0.36302706,  0.10020231]), 'y': 3, 'purity': 0.8951048951048951}\n",
      "{'ref': array([-1.25512808,  0.22794632, -0.85663716, -0.51872942, -0.05145264]), 'y': 1, 'purity': 0.8636363636363636}\n",
      "{'ref': array([-0.41901133, -0.73939077, -0.36445396,  1.33509654,  0.00833069]), 'y': 5, 'purity': 0.9007633587786259}\n",
      "Accuracy: 17.0\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, df = get_data()\n",
    "# df is pandas dataframe before standardizing data\n",
    "# y_test and X_test are Category data and style can be found above\n",
    "# X_train and y_train cols are feature cols in the order\n",
    "# Currently Feature Cols are: ['Time', 'Time_Class', 'DayOfWeek', 'X', 'Y']\n",
    "print(\"Running PCA\")\n",
    "std_data, vals, vectors, std, mean = pca_vv(X_train)\n",
    "z2, r2 = pca_dr(std_data, vals, vectors, 5)\n",
    "\n",
    "print(\"Running K-Means Clustering\")\n",
    "kmeans = KMeans(n_clusters=6, random_state=0).fit(z2)\n",
    "refs = myKMeans(z2, y_train, 6)\n",
    "print(\"Clustering has ended!\")\n",
    "print(\"\\nPurity of Refs:\")\n",
    "for ref in refs:\n",
    "    print(ref)\n",
    "\n",
    "train_, test_ = standardize_data(X_train, X_test, numpy_array=True)\n",
    "test_z2, test_r2 = pca_dr(test_, vals, vectors, 5)\n",
    "test_model(refs, test_z2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using my K-Means Algorithm with PCA d=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCA\n",
      "Running K-Means Clustering\n",
      "Clustering has ended!\n",
      "\n",
      "Purity of Refs:\n",
      "{'ref': array([-0.21189379, -1.94959232,  0.66694336, -0.89039051]), 'y': 2, 'purity': 0.8658536585365854}\n",
      "{'ref': array([2.70635246, 0.12277801, 0.12771246, 0.07207263]), 'y': 4, 'purity': 0.9}\n",
      "{'ref': array([-0.69940433,  0.75958174,  0.82436274,  0.01283566]), 'y': 0, 'purity': 0.8715596330275229}\n",
      "{'ref': array([ 0.45714505,  0.37190388, -0.70374815, -0.36247766]), 'y': 3, 'purity': 0.8958333333333334}\n",
      "{'ref': array([-1.25512808,  0.22794632, -0.85663716, -0.51872942]), 'y': 1, 'purity': 0.8636363636363636}\n",
      "{'ref': array([-0.41896949, -0.74413571, -0.36618438,  1.3438724 ]), 'y': 5, 'purity': 0.9}\n",
      "Accuracy: 17.0\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, df = get_data()\n",
    "# df is pandas dataframe before standardizing data\n",
    "# y_test and X_test are Category data and style can be found above\n",
    "# X_train and y_train cols are feature cols in the order\n",
    "# Currently Feature Cols are: ['Time', 'Time_Class', 'DayOfWeek', 'X', 'Y']\n",
    "print(\"Running PCA\")\n",
    "std_data, vals, vectors, std, mean = pca_vv(X_train)\n",
    "z2, r2 = pca_dr(std_data, vals, vectors, 4)\n",
    "\n",
    "print(\"Running K-Means Clustering\")\n",
    "kmeans = KMeans(n_clusters=6, random_state=0).fit(z2)\n",
    "refs = myKMeans(z2, y_train, 6)\n",
    "print(\"Clustering has ended!\")\n",
    "print(\"\\nPurity of Refs:\")\n",
    "for ref in refs:\n",
    "    print(ref)\n",
    "\n",
    "train_, test_ = standardize_data(X_train, X_test, numpy_array=True)\n",
    "test_z2, test_r2 = pca_dr(test_, vals, vectors, 4)\n",
    "test_model(refs, test_z2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using my K-Means Algorithm with PCA d=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCA\n",
      "Running K-Means Clustering\n",
      "Clustering has ended!\n",
      "\n",
      "Purity of Refs:\n",
      "{'ref': array([0.53453715, 0.20274272, 0.76409528]), 'y': 2, 'purity': 0.875}\n",
      "{'ref': array([2.73874222, 0.08297756, 0.10602244]), 'y': 4, 'purity': 0.8981481481481481}\n",
      "{'ref': array([-1.02968585,  0.81965482,  0.77538419]), 'y': 1, 'purity': 0.8392857142857143}\n",
      "{'ref': array([ 0.31117579,  0.1708872 , -0.98717796]), 'y': 0, 'purity': 0.91875}\n",
      "{'ref': array([-1.31345199,  0.03499478, -0.7710587 ]), 'y': 3, 'purity': 0.8896551724137931}\n",
      "{'ref': array([-0.40446125, -1.98681706,  0.47260751]), 'y': 5, 'purity': 0.8811881188118812}\n",
      "Accuracy: 15.0\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, df = get_data()\n",
    "# df is pandas dataframe before standardizing data\n",
    "# y_test and X_test are Category data and style can be found above\n",
    "# X_train and y_train cols are feature cols in the order\n",
    "# Currently Feature Cols are: ['Time', 'Time_Class', 'DayOfWeek', 'X', 'Y']\n",
    "print(\"Running PCA\")\n",
    "std_data, vals, vectors, std, mean = pca_vv(X_train)\n",
    "z2, r2 = pca_dr(std_data, vals, vectors, 3)\n",
    "\n",
    "print(\"Running K-Means Clustering\")\n",
    "kmeans = KMeans(n_clusters=6, random_state=0).fit(z2)\n",
    "refs = myKMeans(z2, y_train, 6)\n",
    "print(\"Clustering has ended!\")\n",
    "print(\"\\nPurity of Refs:\")\n",
    "for ref in refs:\n",
    "    print(ref)\n",
    "\n",
    "train_, test_ = standardize_data(X_train, X_test, numpy_array=True)\n",
    "test_z2, test_r2 = pca_dr(test_, vals, vectors, 3)\n",
    "test_model(refs, test_z2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using my K-Means Algorithm with PCA d=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCA\n",
      "Running K-Means Clustering\n",
      "Clustering has ended!\n",
      "\n",
      "Purity of Refs:\n",
      "{'ref': array([ 0.97889994, -1.37481099]), 'y': 5, 'purity': 0.9036144578313253}\n",
      "{'ref': array([2.70146454, 0.3091917 ]), 'y': 4, 'purity': 0.8942307692307693}\n",
      "{'ref': array([-1.32399896,  0.91299239]), 'y': 2, 'purity': 0.8553459119496856}\n",
      "{'ref': array([0.34867606, 0.64533715]), 'y': 0, 'purity': 0.8829268292682927}\n",
      "{'ref': array([-0.87402448, -0.28193642]), 'y': 1, 'purity': 0.9058823529411765}\n",
      "{'ref': array([-1.0058562 , -2.00628626]), 'y': 3, 'purity': 0.8493150684931506}\n",
      "Accuracy: 13.0\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, df = get_data()\n",
    "# df is pandas dataframe before standardizing data\n",
    "# y_test and X_test are Category data and style can be found above\n",
    "# X_train and y_train cols are feature cols in the order\n",
    "# Currently Feature Cols are: ['Time', 'Time_Class', 'DayOfWeek', 'X', 'Y']\n",
    "print(\"Running PCA\")\n",
    "std_data, vals, vectors, std, mean = pca_vv(X_train)\n",
    "z2, r2 = pca_dr(std_data, vals, vectors, 2)\n",
    "\n",
    "print(\"Running K-Means Clustering\")\n",
    "kmeans = KMeans(n_clusters=6, random_state=0).fit(z2)\n",
    "refs = myKMeans(z2, y_train, 6)\n",
    "print(\"Clustering has ended!\")\n",
    "print(\"\\nPurity of Refs:\")\n",
    "for ref in refs:\n",
    "    print(ref)\n",
    "\n",
    "train_, test_ = standardize_data(X_train, X_test, numpy_array=True)\n",
    "test_z2, test_r2 = pca_dr(test_, vals, vectors, 2)\n",
    "test_model(refs, test_z2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SKLearn K-Means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 0, 2, 2, 5, 3, 2, 0, 3, 4, 3, 3, 3, 2, 5, 0, 1, 0, 0, 5, 3,\n",
       "       2, 3, 2, 3, 1, 4, 5, 3, 5, 3, 5, 2, 4, 0, 0, 5, 3, 0, 5, 2, 5, 3,\n",
       "       3, 3, 2, 0, 3, 2, 0, 2, 1, 0, 1, 0, 5, 2, 0, 3, 3, 1, 1, 0, 1, 5,\n",
       "       1, 2, 0, 0, 5, 3, 0, 1, 3, 4, 3, 5, 0, 0, 5, 3, 3, 3, 3, 5, 3, 1,\n",
       "       0, 3, 0, 2, 3, 2, 2, 2, 3, 0, 1, 0, 3, 3, 3, 3, 3, 3, 1, 0, 0, 3,\n",
       "       3, 3, 4, 3, 3, 0, 4, 3, 0, 2, 3, 5, 0, 0, 3, 2, 4, 5, 0, 0, 3, 5,\n",
       "       0, 0, 3, 2, 2, 2, 5, 3, 0, 0, 3, 0, 5, 0, 4, 0, 3, 1, 2, 3, 3, 5,\n",
       "       4, 0, 3, 3, 4, 3, 3, 0, 0, 0, 3, 1, 0, 5, 3, 3, 1, 4, 3, 5, 0, 2,\n",
       "       0, 5, 1, 3, 2, 1, 0, 0, 5, 3, 3, 2, 4, 1, 3, 0, 2, 3, 3, 3, 0, 3,\n",
       "       5, 2], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_, test_ = standardize_data(X_train, X_test, numpy_array=True)\n",
    "test_z2, test_r2 = pca_dr(test_, vals, vectors, 2)\n",
    "kmeans.predict(test_z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 5, 4, 5, 5, 0, 0, 5, 5, 5, 4, 5, 5, 5, 5, 5, 2, 2, 0, 5, 0,\n",
       "       5, 2, 5, 5, 2, 1, 5, 5, 5, 2, 0, 4, 1, 2, 5, 5, 2, 5, 5, 5, 5, 0,\n",
       "       5, 0, 3, 5, 0, 5, 5, 5, 5, 5, 0, 0, 3, 4, 5, 3, 0, 5, 1, 1, 5, 5,\n",
       "       5, 5, 3, 2, 0, 5, 0, 1, 0, 5, 5, 5, 5, 5, 5, 4, 3, 0, 5, 1, 4, 5,\n",
       "       5, 5, 5, 5, 2, 5, 5, 1, 5, 5, 5, 1, 5, 5, 0, 5, 5, 5, 5, 1, 1, 5,\n",
       "       5, 1, 5, 0, 5, 5, 5, 5, 5, 0, 5, 5, 3, 0, 0, 5, 5, 0, 0, 3, 5, 0,\n",
       "       5, 5, 5, 0, 5, 5, 4, 2, 5, 0, 5, 2, 5, 0, 2, 2, 5, 5, 5, 2, 0, 5,\n",
       "       1, 3, 0, 0, 0, 0, 1, 5, 2, 0, 1, 1, 3, 5, 5, 4, 1, 5, 5, 5, 1, 5,\n",
       "       5, 0, 5, 5, 4, 2, 2, 5, 1, 5, 3, 2, 1, 3, 0, 5, 5, 1, 5, 5, 1, 0,\n",
       "       1, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
